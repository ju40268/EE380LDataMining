{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\">Due: Thursday, April 24th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Chia-Ju, Chen\n",
    "## UT EID: cc65542\n",
    "\n",
    "## Name: Minke Cheng\n",
    "## UT EID: mc65568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forest vs Boosting - Regression (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will compare performance of different ensemble methods for regression problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor). We will use the Appliances energy prediction dataset for this problem https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction. Use random_state = 42 for the algorithms.\n",
    "\n",
    "1. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Find the best parameters (*n_estimators*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the accuracy of your model in terms of RMSE. (5pts)\n",
    "\n",
    "2. Use [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR). Find the best parameters (including *n_estimators,* *max_depth* and* learning_rate*), and report corresponding RMSE for each algorithm. (5pts)\n",
    "\n",
    "3. Use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for predicting the targets. Find the best parameters (*n_estimators*, *learning_rate*). Report the accuracy of your model in terms of RMSE. (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13222, 56) (13222,) (6513, 56) (6513,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('energydata_complete.csv') \n",
    "\n",
    "y = data['Appliances']\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis = 1)\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "timeData = np.array(data['date'])\n",
    "\n",
    "days = []\n",
    "hours = []\n",
    "for line in range(len(timeData)):\n",
    "    day = parser.parse(timeData[line]).weekday()\n",
    "    hour = parser.parse(timeData[line]).hour\n",
    "    days.append(day)\n",
    "    hours.append(hour)\n",
    "    \n",
    "X = pd.concat([X, pd.get_dummies(days), pd.get_dummies(hours)], axis = 1)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----- rg ---- done ----- \n",
      " ----- train model ---- done -----\n",
      "Random Forest: {'n_estimators': 250}\n",
      "Random Forest - Root Mean Sqrt Error:  71.223991810536\n"
     ]
    }
   ],
   "source": [
    "# 5 - 1\n",
    "\n",
    "param_list = [10, 50, 100, 150, 200, 250]\n",
    "parameters  = {'n_estimators':param_list}\n",
    "random_forest = RandomForestRegressor(random_state=42)\n",
    "rg = GridSearchCV(random_forest, parameters)\n",
    "print(\" ----- rg ---- done ----- \")\n",
    "model = rg.fit(x_train, y_train)\n",
    "print(\" ----- train model ---- done -----\")\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model.predict(x_test)))\n",
    "print('Random Forest:', model.best_params_ )\n",
    "print('Random Forest - Root Mean Sqrt Error: ',rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- Now working with: ----  0 0\n",
      " ---- Now working with: ----  0 1\n",
      " ---- Now working with: ----  0 2\n",
      " ---- Now working with: ----  0 3\n",
      " ---- Now working with: ----  0 4\n",
      " ---- Now working with: ----  1 0\n",
      " ---- Now working with: ----  1 1\n",
      " ---- Now working with: ----  1 2\n",
      " ---- Now working with: ----  1 3\n",
      " ---- Now working with: ----  1 4\n",
      " ---- Now working with: ----  2 0\n",
      " ---- Now working with: ----  2 1\n",
      " ---- Now working with: ----  2 2\n",
      " ---- Now working with: ----  2 3\n",
      " ---- Now working with: ----  2 4\n",
      " ---- Now working with: ----  3 0\n",
      " ---- Now working with: ----  3 1\n",
      " ---- Now working with: ----  3 2\n",
      " ---- Now working with: ----  3 3\n",
      " ---- Now working with: ----  3 4\n",
      " ---- Now working with: ----  4 0\n",
      " ---- Now working with: ----  4 1\n",
      " ---- Now working with: ----  4 2\n",
      " ---- Now working with: ----  4 3\n",
      " ---- Now working with: ----  4 4\n",
      " ----- best params: -------- \n",
      " ----- learning rate: ------ \n",
      "0.5\n",
      " ----- max depth: ------ \n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 5 - 2\n",
    "# including n_estimators, max_depth and learning_rate, and report corresponding RMSE for each algorithm.\n",
    "max_depth = [1,2,3,4,5]\n",
    "all_rmse = [0] * 25\n",
    "learning_rate = [0.01,0.05,0.1,0.5,1]\n",
    "\n",
    "\n",
    "# total should have 5 * 5 = 25 vals\n",
    "# [], [], [], [] ... for the first list, should have max_depth1, learning_rate=0.01\n",
    "for i in range(len(max_depth)):\n",
    "    for j in range(len(learning_rate)):\n",
    "        gradient_regressor = GradientBoostingRegressor(learning_rate=learning_rate[j], max_depth=max_depth[i])\n",
    "        gradient_regressor.fit(x_train, y_train)\n",
    "        print(' ---- Now working with: ---- ', i, j)\n",
    "        all_rmse[i * (len(learning_rate)) + j] = np.sqrt(mean_squared_error(y_test, gradient_regressor.predict(x_test)))\n",
    "\n",
    "best_rmse = all_rmse.index(min(all_rmse))\n",
    "\n",
    "print(' ----- best params: -------- ')\n",
    "print(' ----- learning rate: ------ ')\n",
    "print(learning_rate[best_rmse % len(learning_rate)])\n",
    "print(' ----- max depth: ------ ')\n",
    "print(max_depth[best_rmse // len(max_depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- grid search ----- done ---- \n",
      " --- train ----- done ---- \n",
      " ---- Gradient Boosting Regressor: ----  {'n_estimators': 300}\n",
      " ---- Gradient Boosting Regressor : Root Mean Sqrt Error ----  75.57619087579519\n"
     ]
    }
   ],
   "source": [
    "# 5 - 2\n",
    "parameters = {'n_estimators':[100,200,300]}\n",
    "random_forest_5_2 = GradientBoostingRegressor(learning_rate=0.5, max_depth=5)\n",
    "rg2 = GridSearchCV(random_forest_5_2, parameters)\n",
    "print(' --- grid search ----- done ---- ')\n",
    "model2 = rg2.fit(x_train, y_train)\n",
    "print(' --- train ----- done ---- ')\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model2.predict(x_test)))\n",
    "print(' ---- Gradient Boosting Regressor: ---- ', model2.best_params_)\n",
    "print(' ---- Gradient Boosting Regressor : Root Mean Sqrt Error ---- ',rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- all rmse ----- \n",
      "[96.87643986929898, 98.1491196229277, 105.07778835671895, 166.59566109610932, 222.0434574026829, 417.1137414860408, 106.11990938908293]\n",
      "------ best learning rate ------ \n",
      "0.01\n",
      " ---- grid search ----- done ----- \n",
      " ---- train ---- done ----- \n",
      " ---- Adaboost ----   {'n_estimators': 100}\n",
      " ---- Adaboost Root Mean Sqrt Error ----  96.81503919028403\n"
     ]
    }
   ],
   "source": [
    "# 5 - 3\n",
    "# Use AdaBoost for predicting the targets. Find the best parameters (n_estimators, learning_rate). Report the accuracy of your model in terms of RMSE. \n",
    "learning_rate_adaboost = [0.01,0.05,0.1,0.5,1,5,10]\n",
    "all_rmse_5_3 = [0] * len(learning_rate_adaboost)\n",
    "\n",
    "for i in range(len(learning_rate_adaboost)):\n",
    "    ada_reg = AdaBoostRegressor(learning_rate=learning_rate_adaboost[i])\n",
    "    ada_reg.fit(x_train, y_train)\n",
    "    all_rmse_5_3[i] = np.sqrt(mean_squared_error(y_test, ada_reg.predict(x_test)))\n",
    "print(' ---- all rmse ----- ')\n",
    "print(all_rmse_5_3)\n",
    "print('------ best learning rate ------ ')\n",
    "print(learning_rate_adaboost[all_rmse_5_3.index(min(all_rmse_5_3))])\n",
    "               \n",
    "best_learning_rate_adaboost = learning_rate_adaboost[all_rmse_5_3.index(min(all_rmse_5_3))]\n",
    "\n",
    "parameters = {'n_estimators':[50, 100, 150, 200]}\n",
    "random_forest_5_3 = AdaBoostRegressor(learning_rate=best_learning_rate_adaboost)\n",
    "rg3 = GridSearchCV(random_forest_5_3, parameters)\n",
    "print(' ---- grid search ----- done ----- ')\n",
    "model3 = rg3.fit(x_train, y_train)\n",
    "print(' ---- train ---- done ----- ')\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model3.predict(x_test)))\n",
    "print(' ---- Adaboost ----  ',model3.best_params_)\n",
    "print(' ---- Adaboost Root Mean Sqrt Error ---- ',rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Random Forest vs Boosting - Classification (15 pts)\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). As in HW4, you will be classifying signal vs background in the MAGIC Gamma Telescope Data Set. The data has been split into training and test as well as standardized for you.\n",
    "\n",
    "1. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for the classification. Set the random_state to 42. Find the best parameters (by varying *n_estimators* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. (4pts)\n",
    "\n",
    "2. Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Again, find the best parameters (including *n_estimators, learning_rate,* and *max_depth (GBDT only)*), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm. (8pts)\n",
    "\n",
    "3. Point out one advantage and one disadvantage of Random Forest compared to GBDT (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmk/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# train\n",
    "train = pd.read_csv('magic_train.csv', header=None)\n",
    "y_train = train.values[:,10]\n",
    "y_train[y_train == 'g'] = 0\n",
    "y_train[y_train == 'h'] = 1\n",
    "y_train = y_train.astype(float)\n",
    "x_train = train.values[:,:10]\n",
    "\n",
    "# test\n",
    "test = pd.read_csv('magic_test.csv', header=None)\n",
    "y_test = test.values[:,10]\n",
    "y_test[y_test == 'g'] = 0\n",
    "y_test[y_test == 'h'] = 1\n",
    "y_test = y_test.astype(float)\n",
    "x_test = test.values[:,:10]\n",
    "\n",
    "# standardize the data\n",
    "x_train = preprocessing.scale(x_train)\n",
    "x_test = preprocessing.scale(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 50, 100, 150, 200, 250], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [10, 50, 100, 150, 200, 250], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "RF_model = RandomForestClassifier(random_state=42)\n",
    "RF_Grid = GridSearchCV(RF_model, param_grid)\n",
    "RF_Grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators for Random Forest is: 150\n",
      "Best criterion for Random Forest is: gini\n"
     ]
    }
   ],
   "source": [
    "print(\"Best n_estimators for Random Forest is: \" + str(RF_Grid.best_params_[\"n_estimators\"]))\n",
    "print(\"Best criterion for Random Forest is: \" + str(RF_Grid.best_params_[\"criterion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (accuracy_score) for Random Forest is: 0.876657362177\n",
      "Roc_auc_score for Random Forest is: 0.932605726673\n"
     ]
    }
   ],
   "source": [
    "y_predict = RF_Grid.predict(x_test)\n",
    "a_score = accuracy_score(y_test, y_predict)\n",
    "\n",
    "roc_score = roc_auc_score(y_test, RF_Grid.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Test accuracy (accuracy_score) for Random Forest is: \" + str(a_score))\n",
    "print(\"Roc_auc_score for Random Forest is: \" + str(roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 50, 100, 150, 200, 250], 'max_depth': [1, 2, 3, 4, 5], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [10, 50, 100, 150, 200, 250], \"max_depth\": [1, 2, 3, 4, 5], \"learning_rate\": [0.01, 0.05, 0.1, 0.5, 1]}\n",
    "GBDT_model = GradientBoostingClassifier(random_state=42)\n",
    "GBDT_Grid = GridSearchCV(GBDT_model, param_grid)\n",
    "GBDT_Grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators for GBDT is: 250\n",
      "Best learning_rate for GBDT is: 0.1\n",
      "Best max_depth for GBDT is: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Best n_estimators for GBDT is: \" + str(GBDT_Grid.best_params_[\"n_estimators\"]))\n",
    "print(\"Best learning_rate for GBDT is: \" + str(GBDT_Grid.best_params_[\"learning_rate\"]))\n",
    "print(\"Best max_depth for GBDT is: \" + str(GBDT_Grid.best_params_[\"max_depth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (accuracy_score) for GBDT is: 0.875610607118\n",
      "Roc_auc_score for GBDT is: 0.932371868742\n"
     ]
    }
   ],
   "source": [
    "y_predict = GBDT_Grid.predict(x_test)\n",
    "a_score = accuracy_score(y_test, y_predict)\n",
    "\n",
    "roc_score = roc_auc_score(y_test, GBDT_Grid.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Test accuracy (accuracy_score) for GBDT is: \" + str(a_score))\n",
    "print(\"Roc_auc_score for GBDT is: \" + str(roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 50, 100, 150, 200, 250], 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\": [10, 50, 100, 150, 200, 250], \"learning_rate\": [0.01, 0.05, 0.1, 0.5, 1]}\n",
    "Ada_model = AdaBoostClassifier(random_state=42)\n",
    "Ada_Grid = GridSearchCV(Ada_model, param_grid)\n",
    "Ada_Grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators for AdaBoost is: 200\n",
      "Best learning_rate for AdaBoost is: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Best n_estimators for AdaBoost is: \" + str(Ada_Grid.best_params_[\"n_estimators\"]))\n",
    "print(\"Best learning_rate for AdaBoost is: \" + str(Ada_Grid.best_params_[\"learning_rate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (accuracy_score) for AdaBoost is: 0.847348220516\n",
      "Roc_auc_score for AdaBoost is: 0.900043547332\n"
     ]
    }
   ],
   "source": [
    "y_predict = Ada_Grid.predict(x_test)\n",
    "a_score = accuracy_score(y_test, y_predict)\n",
    "\n",
    "roc_score = roc_auc_score(y_test, Ada_Grid.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Test accuracy (accuracy_score) for AdaBoost is: \" + str(a_score))\n",
    "print(\"Roc_auc_score for AdaBoost is: \" + str(roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of Random Forest compared to GBDT is: Random Forest is easier to parallelize while GBDT is not so trivial to achieve it. For each tree of Random Forest, we use bootstrap method to choose smaples. We also choose a random subset of the festures to build the model. In this case, the training process of different trees will not interact with each other, and thus it is easier to be splitted and computed synchronously in multiple threads. By the way, Random Forest is usually easier to tune.\n",
    "\n",
    "One disadvantage of Random Forest compared to GBDT is: GBDT performs better on weak learners with high bias and low variance. Each step, a new model is built to compenstae the already built model. So if our model is of relatively low accuracy, GBDT can be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the \"nbasalaries_new.csv\" data set, your goal is to build a Bokeh visualization which allows the user to explore how salary (on a log scale) varies with points per game (PSG) and age. You will create a visualization that allows the user to toggle the X axis of a scatter plot between PSG and age, with the y-axis always being log Salary. Also add the hover tool so that if the user hovers over a datapoint in the plot a window pops up that shows the player name, team, position, salary, and the current x variable (PSG or age) depending on the current tab.  Add the ability to Zoom in/out.\n",
    "\n",
    "Hints: \n",
    "1. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for hover and zoom tool examples.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "3. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "4. See: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html  for how to use jitter transform\n",
    "5. Use output_file() from Bokeh to output the plot to an external file. Submit this file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0            Player   Tm Pos   Age     G    GS    MP    FG   FGA  \\\n",
      "0           0     Stephen Curry  GSW  PG  27.0  79.0  79.0  34.2  10.2  20.2   \n",
      "1           1      James Harden  HOU  SG  26.0  82.0  82.0  38.1   8.7  19.7   \n",
      "2           2      Kevin Durant  OKC  SF  27.0  72.0  72.0  35.8   9.7  19.2   \n",
      "3           3  DeMarcus Cousins  SAC   C  25.0  65.0  65.0  34.6   9.2  20.5   \n",
      "4           4      LeBron James  CLE  SF  31.0  76.0  76.0  35.6   9.7  18.6   \n",
      "\n",
      "     ...      DRB   TRB  AST  STL  BLK  TOV   PF   PSG        SALARY  \\\n",
      "0    ...      4.6   5.4  6.7  2.1  0.2  3.3  2.0  30.1  1.136834e+07   \n",
      "1    ...      5.3   6.1  7.5  1.7  0.6  4.6  2.8  29.0  1.575603e+07   \n",
      "2    ...      7.6   8.2  5.0  1.0  1.2  3.5  1.9  28.2  2.016125e+07   \n",
      "3    ...      9.1  11.5  3.3  1.6  1.4  3.8  3.6  26.9  1.585438e+07   \n",
      "4    ...      6.0   7.4  6.8  1.4  0.6  3.3  1.9  25.3  2.296021e+07   \n",
      "\n",
      "   logsalary  \n",
      "0  16.246343  \n",
      "1  16.572734  \n",
      "2  16.819273  \n",
      "3  16.578956  \n",
      "4  16.949273  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, BoxZoomTool, Jitter\n",
    "from bokeh.plotting import figure, output_notebook, show, output_file\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "data = pd.read_csv(\"nbasalaries_new.csv\")\n",
    "data[\"logsalary\"] = data.SALARY.apply(np.log)\n",
    "print(data.head())\n",
    "x_psg = data['PSG']\n",
    "x_age = data['Age']\n",
    "df_position = data['Pos']\n",
    "df_salary = data['SALARY']\n",
    "df_name = data['Player']\n",
    "log_salary = data['logsalary']\n",
    "\n",
    "# PSG\n",
    "# player name, team, position, salary, and the current x variable (PSG or age) \n",
    "# depending on the current tab. Add the ability to Zoom in/out.\n",
    "source_psg = ColumnDataSource(data=dict(\n",
    "    x= [x for x in x_psg],\n",
    "    y= [y for y in log_salary],\n",
    "    name = [n for n in df_name],\n",
    "    position = [p for p in df_position],\n",
    "    salary = [s for s in df_salary]\n",
    "))\n",
    "hover_psg = HoverTool(tooltips=[\n",
    "    (\"index\", \"$index\"),\n",
    "    (\"(x,y)\", \"($x, $y)\"),\n",
    "    (\"Player\", \"@name\"),\n",
    "    (\"Postion\", \"@position\"),\n",
    "    (\"Salary\", \"@salary\"),\n",
    "])\n",
    "p_psg = figure(tools=[hover_psg, BoxZoomTool(),\"reset\", \"zoom_in\", \"zoom_out\"], title=\"HW5_3 Scatter Plot (PSG to Salary)\")\n",
    "p_psg.border_fill_color = \"whitesmoke\"\n",
    "p_psg.xaxis.axis_label = \"points per game (PSG)\"\n",
    "p_psg.yaxis.axis_label = \"Salary(Log Scale)\"\n",
    "p_psg.scatter(x_psg, log_salary, fill_alpha=0.6, line_color=None)\n",
    "p_psg.circle('x', 'y', source=source_psg)\n",
    "tab1 = Panel(child=p_psg, title=\"PSG_Salary\")\n",
    "\n",
    "# Age\n",
    "\n",
    "source_age = ColumnDataSource(data=dict(\n",
    "    x= [x for x in x_age],\n",
    "    y= [y for y in log_salary],\n",
    "    name = [n for n in df_name],\n",
    "    position = [p for p in df_position],\n",
    "    salary = [s for s in df_salary]\n",
    "))\n",
    "\n",
    "hover_age = HoverTool(tooltips=[\n",
    "    (\"index\", \"$index\"),\n",
    "    (\"(x,y)\", \"($x, $y)\"),\n",
    "    (\"Player\", \"@name\"),\n",
    "    (\"Postion\", \"@position\"),\n",
    "    (\"Salary\", \"@salary\"),\n",
    "])\n",
    "p_age = figure(tools=[hover_age, BoxZoomTool(),\"reset\", \"zoom_in\", \"zoom_out\"], title=\"HW5_3 Scatter Plot (Salary to Age)\")\n",
    "p_age.border_fill_color = \"whitesmoke\"\n",
    "p_age.xaxis.axis_label = \"Age\"\n",
    "p_age.yaxis.axis_label = \"Salary(Log Scale)\"\n",
    "p_age.scatter(x_age, log_salary,fill_alpha=0.6, line_color=None)\n",
    "p_age.circle('x', 'y', source=source_age)\n",
    "tab2 = Panel(child=p_age, title=\"Age_Salary\")\n",
    "tabs = Tabs(tabs=[ tab1, tab2 ])\n",
    "output_file(\"scatter.html\", title=\"color_scatter.py example\")\n",
    "\n",
    "show(tabs)  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
